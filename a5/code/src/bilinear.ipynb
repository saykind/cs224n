{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bilinear attention formulation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import random\n",
    "import argparse\n",
    "random.seed(0)\n",
    "\n",
    "import dataset\n",
    "import model\n",
    "import trainer\n",
    "import utils\n",
    "\n",
    "# Save the device\n",
    "device = torch.cuda.current_device() if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python src/run.py pretrain vanilla wiki.txt --writing_params_path vanilla.pretrain.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 418352 characters, 256 unique.\n"
     ]
    }
   ],
   "source": [
    "tb_expt_name = 'ipynb'\n",
    "function = 'pretrain'\n",
    "variant = 'vanilla'\n",
    "pretrain_corpus_path = '../wiki.txt'\n",
    "bottleneck_dim = 32\n",
    "pretrain_lr = 6e-3\n",
    "finetune_lr = 6e-4\n",
    "\n",
    "# TensorBoard training log\n",
    "writer = SummaryWriter(log_dir='expt/%s/%s_%s_%d_pt_lr_%f_ft_lr_%f' % (\n",
    "    function,\n",
    "    tb_expt_name,\n",
    "    variant,\n",
    "    bottleneck_dim,\n",
    "    pretrain_lr,\n",
    "    finetune_lr))\n",
    "\n",
    "# Keep the block size 128\n",
    "# Why is the pretraining corpus always required (even if we're not pretraining?)\n",
    "# It's because we're using it as a hack to always have the same vocabulary\n",
    "# (that is, the same mapping from character to integer, and we build the\n",
    "# vocab from the pretraining corpus.)\n",
    "block_size = 128\n",
    "text = open(pretrain_corpus_path, encoding='utf-8').read()\n",
    "pretrain_dataset = dataset.CharCorruptionDataset(text, block_size)\n",
    "\n",
    "# We don't suggest you change these hyperparameters, as they're known to work.\n",
    "# use them for both the vanilla and the perceiver models\n",
    "mconf = model.GPTConfig(pretrain_dataset.vocab_size, pretrain_dataset.block_size,\n",
    "    n_layer=4, n_head=8, n_embd=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 3323392\n",
      "Model on device:  cpu\n"
     ]
    }
   ],
   "source": [
    "# construct a GPT model\n",
    "model = model.GPT(mconf)\n",
    "model.to(device)\n",
    "\n",
    "print('Model on device: ', next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vanilla, number of parameters: 3323392"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python src/run.py pretrain bilinear wiki.txt --writing_params_path bilinear.pretrain.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 418352 characters, 256 unique.\n"
     ]
    }
   ],
   "source": [
    "tb_expt_name = 'ipynb'\n",
    "function = 'pretrain'\n",
    "variant = 'bilinear'\n",
    "pretrain_corpus_path = '../wiki.txt'\n",
    "bottleneck_dim = 32\n",
    "pretrain_lr = 6e-3\n",
    "finetune_lr = 6e-4\n",
    "\n",
    "# TensorBoard training log\n",
    "writer = SummaryWriter(log_dir='expt/%s/%s_%s_%d_pt_lr_%f_ft_lr_%f' % (\n",
    "    function,\n",
    "    tb_expt_name,\n",
    "    variant,\n",
    "    bottleneck_dim,\n",
    "    pretrain_lr,\n",
    "    finetune_lr))\n",
    "\n",
    "# Keep the block size 128\n",
    "# Why is the pretraining corpus always required (even if we're not pretraining?)\n",
    "# It's because we're using it as a hack to always have the same vocabulary\n",
    "# (that is, the same mapping from character to integer, and we build the\n",
    "# vocab from the pretraining corpus.)\n",
    "block_size = 128\n",
    "text = open(pretrain_corpus_path, encoding='utf-8').read()\n",
    "pretrain_dataset = dataset.CharCorruptionDataset(text, block_size)\n",
    "\n",
    "# We don't suggest you change these hyperparameters, as they're known to work.\n",
    "# use them for both the vanilla and the perceiver models\n",
    "mconf = model.GPTConfig(pretrain_dataset.vocab_size, pretrain_dataset.block_size,\n",
    "    n_layer=4, n_head=8, n_embd=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 3060224\n",
      "Model on device:  cpu\n"
     ]
    }
   ],
   "source": [
    "# construct a GPT model\n",
    "mconf.bilinear = True\n",
    "model = model.GPT(mconf)\n",
    "model.to(device)\n",
    "\n",
    "print('Model on device: ', next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bilinear, number of parameters: 3060224\n",
    "Which is 8% less than vanilla model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 418352 characters, 256 unique.\n"
     ]
    }
   ],
   "source": [
    "assert pretrain_corpus_path is not None\n",
    "# Open the corpus\n",
    "pretrain_text = open(pretrain_corpus_path, encoding='utf-8').read()\n",
    "# Create a dataset\n",
    "pretrain_dataset = dataset.CharCorruptionDataset(pretrain_text, block_size)\n",
    "# Initialize the trainer\n",
    "tconf = trainer.TrainerConfig(max_epochs=650,\n",
    "                                batch_size=128,\n",
    "                                learning_rate=pretrain_lr,\n",
    "                                lr_decay=True,\n",
    "                                warmup_tokens=512*20,\n",
    "                                final_tokens=200*len(pretrain_dataset)*block_size,\n",
    "                                num_workers=0,\n",
    "                                writer=writer)\n",
    "trainer = trainer.Trainer(model, pretrain_dataset, None, tconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the resulting model in args.writing_params_path\n",
    "writing_params_path = 'bilinear.pretrain.params'\n",
    "torch.save(model.state_dict(), writing_params_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n_dfp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
